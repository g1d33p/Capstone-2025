{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Running EDA...\n",
      "Exporting cleaned data and splits...\n",
      "\n",
      "Training models...\n",
      "\n",
      "--- Logistic Regression Models ---\n",
      "\n",
      "=== LR ===\n",
      "AUC: 0.8009640047879617 PR-AUC: 0.4613650187381289\n",
      "At 10% calling budget: capture=0.456, lift=4.56, threshold=0.7703\n",
      "\n",
      "=== LR_ROS ===\n",
      "AUC: 0.8013884794094062 PR-AUC: 0.4618251201772269\n",
      "At 10% calling budget: capture=0.456, lift=4.56, threshold=0.7712\n",
      "\n",
      "--- Random Forest Models ---\n",
      "\n",
      "=== RF ===\n",
      "AUC: 0.8027208683192603 PR-AUC: 0.4826490668581546\n",
      "At 10% calling budget: capture=0.483, lift=4.83, threshold=0.5420\n",
      "\n",
      "=== RF_ROS ===\n",
      "AUC: 0.795823874357281 PR-AUC: 0.460743952335288\n",
      "At 10% calling budget: capture=0.458, lift=4.58, threshold=0.5912\n",
      "\n",
      "--- XGBoost Models ---\n",
      "\n",
      "=== XGB ===\n",
      "AUC: 0.8009905390584461 PR-AUC: 0.4787433144341429\n",
      "At 10% calling budget: capture=0.482, lift=4.82, threshold=0.7317\n",
      "\n",
      "Exporting ML-ready datasets...\n",
      "\n",
      "Running scenario analysis with modern macroeconomic data...\n",
      "\n",
      "Generating performance summary...\n",
      "Performance summary saved to: performance_summary.csv\n",
      "\n",
      "✅ Complete! All artifacts written to: /Users/jeevandeep/Desktop/capstone_outputs_complete\n",
      "Models trained: LR, LR+ROS, RF, RF+ROS, XGBoost\n",
      "Scenarios tested: Historic (2008-2013) vs Modern (2023-2025)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Capstone Report Pipeline (single-file) - COMPLETE VERSION\n",
    "Author: Jeevan Deep Borugadda\n",
    "\n",
    "Implements ALL steps described in the report including SMOTE, XGBoost weighting, and modern macro integration.\n",
    "\n",
    "Run:\n",
    "    python capstone_report_pipeline.py\n",
    "\n",
    "Requires:\n",
    "    numpy, pandas, scikit-learn, matplotlib, xgboost, imbalanced-learn\n",
    "\"\"\"\n",
    "\n",
    "# ==== Imports & config ====\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_fscore_support,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix, brier_score_loss\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Imbalanced learning for SMOTE\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "    HAS_IMBLEARN = True\n",
    "except Exception:\n",
    "    HAS_IMBLEARN = False\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==== Report-aligned constants ====\n",
    "RAW_CSV_PATH = \"bank-additional-full.csv\"  # must be present in working dir\n",
    "OUTDIR = Path(\"capstone_outputs_complete\")  # all artifacts land here\n",
    "\n",
    "CAT_COLS = [\n",
    "    'job','marital','education','default','housing','loan',\n",
    "    'contact','month','day_of_week','poutcome'\n",
    "]\n",
    "DROP_LEAKAGE = ['duration']\n",
    "DROP_REDUNDANT = ['nr.employed']  # highly collinear with euribor3m; keep euribor3m\n",
    "SKEWED_NUMS = ['campaign','previous','pdays_non999']  # winsorize 1/99\n",
    "TOP_FRACTION = 0.10  # business operating point (top 10% calling budget)\n",
    "\n",
    "# Modern macroeconomic values (illustrative placeholders for 2023–2025 scenario testing)\n",
    "MODERN_MACROS = {\n",
    "    'euribor3m': 3.5,\n",
    "    'cons.conf.idx': -20.0,\n",
    "    'cons.price.idx': 105.0,\n",
    "    'emp.var.rate': 0.5\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Data Preparation\n",
    "# -----------------------------------------------------------------------------\n",
    "def group_rare(series: pd.Series, min_frac: float = 0.005, preserve: List[str] = ['unknown']) -> pd.Series:\n",
    "    s = series.astype(str).str.lower()\n",
    "    freqs = s.value_counts(normalize=True)\n",
    "    rare = [c for c, f in freqs.items() if f < min_frac and c not in preserve]\n",
    "    return s.replace({c: 'Other' for c in rare})\n",
    "\n",
    "def winsorize_clip(s: pd.Series, low_q=0.01, high_q=0.99) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    lo, hi = s.quantile(low_q), s.quantile(high_q)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Implements report Section 4:\n",
    "    - target y (0/1)\n",
    "    - Drop leakage (duration)\n",
    "    - pdays sentinel → contacted_before, pdays_non999\n",
    "    - Rare-level grouping for categories; keep 'unknown'\n",
    "    - Winsorization for skewed numerics\n",
    "    - Drop redundant macro (nr.employed)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Target\n",
    "    df['y'] = (df['y'].astype(str).str.lower() == 'yes').astype(int)\n",
    "\n",
    "    # Drop leakage\n",
    "    for c in DROP_LEAKAGE:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=c, inplace=True, errors='ignore')\n",
    "\n",
    "    # Sentinel handling\n",
    "    df['contacted_before'] = (df['pdays'] != 999).astype(int)\n",
    "    df['pdays_non999'] = df['pdays'].replace(999, np.nan)\n",
    "\n",
    "    # Rare-level grouping for listed categoricals\n",
    "    for c in CAT_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = group_rare(df[c], min_frac=0.005, preserve=['unknown'])\n",
    "\n",
    "    # Winsorize right-skewed numeric fields\n",
    "    for c in SKEWED_NUMS:\n",
    "        if c in df.columns:\n",
    "            df[c] = winsorize_clip(df[c], 0.01, 0.99)\n",
    "\n",
    "    # Drop redundant macro\n",
    "    for c in DROP_REDUNDANT:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=c, inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Preprocessors & Models\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_preprocessor(X: pd.DataFrame, algo: str) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    Family-specific preprocessing per report:\n",
    "    - LR  : numeric median-impute + StandardScaler; pdays_non999 -> -1 + scale; OHE cats\n",
    "    - RF  : numeric median-impute, NO scaling; pdays_non999 -> -1; OHE cats\n",
    "    - XGB : numeric passthrough (keep NaNs so tree can split on missing), NO scaling; OHE cats\n",
    "    \"\"\"\n",
    "    num_all = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    special = \"pdays_non999\"\n",
    "    other_nums = [c for c in num_all if c != special]\n",
    "\n",
    "    # --- Categorical encoder\n",
    "    try:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    cat_cols_present = [c for c in CAT_COLS if c in X.columns]\n",
    "    cat_tf = Pipeline([(\"ohe\", ohe)])\n",
    "\n",
    "    transformers = []\n",
    "\n",
    "    if algo == \"lr\":\n",
    "        # Other numerics: median + scale\n",
    "        if other_nums:\n",
    "            transformers.append((\"num_other\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]), other_nums))\n",
    "        # pdays_non999: -1 + scale\n",
    "        if special in X.columns:\n",
    "            transformers.append((\"num_pdays\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]), [special]))\n",
    "\n",
    "    elif algo == \"rf\":\n",
    "        # Other numerics: median ONLY (no scaling)\n",
    "        if other_nums:\n",
    "            transformers.append((\"num_other\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            ]), other_nums))\n",
    "        # pdays_non999: -1 (no scaling)\n",
    "        if special in X.columns:\n",
    "            transformers.append((\"num_pdays\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "            ]), [special]))\n",
    "\n",
    "    elif algo == \"xgb\":\n",
    "        # Let XGB learn missing splits: leave numerics as-is (passthrough)\n",
    "        if other_nums:\n",
    "            transformers.append((\"num_other\", \"passthrough\", other_nums))\n",
    "        if special in X.columns:\n",
    "            transformers.append((\"num_pdays\", \"passthrough\", [special]))\n",
    "    else:\n",
    "        raise ValueError(\"algo must be 'lr' | 'rf' | 'xgb'\")\n",
    "\n",
    "    if cat_cols_present:\n",
    "        transformers.append((\"cat\", cat_tf, cat_cols_present))\n",
    "\n",
    "    return ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "def make_model(algo: str, use_resampling: str = None):\n",
    "    if algo == \"lr\":\n",
    "        # With ROS we do not also use class_weight\n",
    "        if use_resampling == \"ros\":\n",
    "            return LogisticRegression(max_iter=500, random_state=RANDOM_SEED)\n",
    "        else:\n",
    "            return LogisticRegression(max_iter=500, class_weight=\"balanced\", random_state=RANDOM_SEED)\n",
    "\n",
    "    \n",
    "    if algo == \"rf\":\n",
    "        if use_resampling == \"ros\":\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=500, max_depth=None, min_samples_leaf=2,\n",
    "                n_jobs=-1, random_state=RANDOM_SEED  # No class_weight with ROS\n",
    "            )\n",
    "        else:\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=500, max_depth=None, min_samples_leaf=2,\n",
    "                class_weight=\"balanced\", n_jobs=-1, random_state=RANDOM_SEED\n",
    "            )\n",
    "    \n",
    "    if algo == \"xgb\":\n",
    "        if not HAS_XGB:\n",
    "            raise ImportError(\"xgboost is not installed. pip install xgboost\")\n",
    "        # Calculate scale_pos_weight for 1:8 imbalance (88.73% / 11.27% ≈ 7.87)\n",
    "        scale_pos_weight = 7.87\n",
    "        return XGBClassifier(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "            subsample=0.8, colsample_bytree=0.8, scale_pos_weight=scale_pos_weight,\n",
    "            random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\"\n",
    "        )\n",
    "    raise ValueError(\"algo must be 'lr' | 'rf' | 'xgb'\")\n",
    "\n",
    "def make_pipeline(X_train: pd.DataFrame, algo: str, use_resampling: str = None) -> Pipeline:\n",
    "    pre = build_preprocessor(X_train, algo)\n",
    "\n",
    "    if use_resampling and HAS_IMBLEARN:\n",
    "        if use_resampling == \"ros\" and algo == \"lr\":\n",
    "            # Random OverSampling for LR (safe with OHE)\n",
    "            ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "            clf = make_model(algo, use_resampling)\n",
    "            return ImbPipeline([\n",
    "                (\"pre\", pre),\n",
    "                (\"ros\", ros),\n",
    "                (\"clf\", clf)\n",
    "            ])\n",
    "        elif use_resampling == \"ros\" and algo == \"rf\":\n",
    "            # Random OverSampling for RF (no class_weight)\n",
    "            ros = RandomOverSampler(random_state=RANDOM_SEED)\n",
    "            clf = make_model(algo, use_resampling)\n",
    "            return ImbPipeline([\n",
    "                (\"pre\", pre),\n",
    "                (\"ros\", ros),\n",
    "                (\"clf\", clf)\n",
    "            ])\n",
    "\n",
    "    \n",
    "    # Default pipeline without resampling\n",
    "    clf = make_model(algo, use_resampling)\n",
    "    return Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Metrics & Plots\n",
    "# -----------------------------------------------------------------------------\n",
    "def decile_metrics(y_true: np.ndarray, y_proba: np.ndarray, top_frac: float = TOP_FRACTION) -> Dict[str, float]:\n",
    "    n = len(y_proba)\n",
    "    k = max(1, int(np.floor(top_frac * n)))\n",
    "    idx = np.argsort(-y_proba)[:k]\n",
    "    captured_positives = int(y_true[idx].sum())\n",
    "    total_positives = int(y_true.sum())\n",
    "    capture_rate = (captured_positives / total_positives) if total_positives > 0 else 0.0\n",
    "    lift = (capture_rate / top_frac) if top_frac > 0 else 0.0\n",
    "    return {\n",
    "        \"decile_top_frac\": top_frac,\n",
    "        \"decile_top_k\": k,\n",
    "        \"decile_captured_positives\": captured_positives,\n",
    "        \"decile_total_positives\": total_positives,\n",
    "        \"decile_capture_rate\": float(capture_rate),\n",
    "        \"decile_lift\": float(lift),\n",
    "        \"decile_calls_saved_percent\": float(1 - top_frac)\n",
    "    }\n",
    "\n",
    "def threshold_for_top_frac(y_proba: np.ndarray, top_frac: float = TOP_FRACTION) -> float:\n",
    "    n = len(y_proba)\n",
    "    k = max(1, int(np.floor(top_frac * n)))\n",
    "    return float(np.sort(y_proba)[::-1][k-1])\n",
    "\n",
    "def eval_classification(y_true: np.ndarray, y_proba: np.ndarray, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_proba)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    brier = brier_score_loss(y_true, y_proba)\n",
    "    return {\n",
    "        \"auc\": float(auc),\n",
    "        \"pr_auc\": float(pr_auc),\n",
    "        f\"precision@{threshold}\": float(prec),\n",
    "        f\"recall@{threshold}\": float(rec),\n",
    "        f\"f1@{threshold}\": float(f1),\n",
    "        \"brier_score\": float(brier)\n",
    "    }\n",
    "\n",
    "def business_point_metrics(y_true: np.ndarray, y_proba: np.ndarray, top_frac: float = TOP_FRACTION) -> Dict[str, float]:\n",
    "    thr = threshold_for_top_frac(y_proba, top_frac)\n",
    "    base = eval_classification(y_true, y_proba, threshold=thr)\n",
    "    deci = decile_metrics(y_true, y_proba, top_frac=top_frac)\n",
    "    return {\"threshold_at_top_frac\": thr, **base, **deci}\n",
    "\n",
    "def make_decile_table(y_true, y_proba, n_deciles: int = 10) -> pd.DataFrame:\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_proba = np.asarray(y_proba).astype(float)\n",
    "    order = np.argsort(-y_proba)\n",
    "    y_sorted = y_true[order]\n",
    "    proba_sorted = y_proba[order]\n",
    "    n = len(y_true)\n",
    "    total_pos = y_true.sum()\n",
    "    idx_splits = np.array_split(np.arange(n), n_deciles)\n",
    "    rows = []\n",
    "    for i, idxs in enumerate(idx_splits, start=1):\n",
    "        pos_in_bin = int(y_sorted[idxs].sum())\n",
    "        pop_frac = len(idxs)/n\n",
    "        capture_in_bin = pos_in_bin/total_pos if total_pos>0 else 0.0\n",
    "        lift = (capture_in_bin/pop_frac) if pop_frac>0 else 0.0\n",
    "        rows.append({\n",
    "            \"decile\": i,\n",
    "            \"n_in_bin\": len(idxs),\n",
    "            \"positives_in_bin\": pos_in_bin,\n",
    "            \"bin_capture_rate\": capture_in_bin,\n",
    "            \"bin_lift\": lift,\n",
    "            \"avg_score\": float(proba_sorted[idxs].mean()) if len(idxs)>0 else np.nan\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values(\"decile\").reset_index(drop=True)\n",
    "\n",
    "def plot_roc(y_true, y_score, title, out_path):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "def plot_pr(y_true, y_score, title, out_path):\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    plt.figure()\n",
    "    plt.plot(rec, prec, label=f\"AP = {ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(title); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "def plot_lift(deciles_df, title, out_path):\n",
    "    plt.figure()\n",
    "    plt.plot(deciles_df[\"decile\"], deciles_df[\"bin_lift\"], marker=\"o\")\n",
    "    plt.xlabel(\"Decile (1 = highest score)\"); plt.ylabel(\"Lift (per decile)\")\n",
    "    plt.title(title); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "def plot_cum_gain(deciles_df, title, out_path):\n",
    "    plt.figure()\n",
    "    plt.plot(deciles_df[\"decile\"], deciles_df[\"bin_capture_rate\"].cumsum() / deciles_df[\"bin_capture_rate\"].sum(), marker=\"o\")\n",
    "    plt.xlabel(\"Decile (1 = highest score)\"); plt.ylabel(\"Cumulative Capture Rate\")\n",
    "    plt.title(title); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "def plot_calibration(y_true, y_score, title, out_path, n_bins=10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_score, n_bins=n_bins, strategy='quantile')\n",
    "    plt.figure()\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\", label=\"Perfect\")\n",
    "    plt.xlabel(\"Predicted probability (binned)\"); plt.ylabel(\"Observed frequency\")\n",
    "    plt.title(title); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            EDA (core figures only)\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_eda_and_save(df: pd.DataFrame, eda_dir: Path):\n",
    "    eda_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dfe = df.copy()\n",
    "    dfe['y_bin'] = dfe['y']\n",
    "\n",
    "    # Figure – Target Distribution (imbalance)\n",
    "    fig = plt.figure()\n",
    "    counts = dfe['y_bin'].value_counts().sort_index()\n",
    "    labels = ['No', 'Yes']\n",
    "    plt.pie(counts, labels=labels, autopct=lambda p: f\"{p:.1f}%\", startangle=90)\n",
    "    plt.title(\"Target Distribution (Imbalance)\")\n",
    "    fig.savefig(eda_dir / \"01_target_distribution.png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    # Figure – % 'Unknown' by Job / Contact (heatmaps)\n",
    "    for group_col, tag in [('job', 'job'), ('contact','contact')]:\n",
    "        cat_cols_with_unknown = ['education', 'default', 'housing', 'loan']\n",
    "        mat = {}\n",
    "        for c in cat_cols_with_unknown:\n",
    "            if c in dfe.columns:\n",
    "                mat[c] = (dfe.assign(is_unknown=(dfe[c].astype(str).str.lower()=='unknown').astype(int))\n",
    "                            .groupby(group_col)['is_unknown'].mean())\n",
    "        if not mat: \n",
    "            continue\n",
    "        mdf = pd.DataFrame(mat).fillna(0.0).sort_index()\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(mdf.values, aspect='auto')\n",
    "        plt.xticks(range(len(mdf.columns)), mdf.columns, rotation=45, ha='right')\n",
    "        plt.yticks(range(len(mdf.index)), mdf.index)\n",
    "        plt.colorbar(label=\"Proportion Unknown\")\n",
    "        plt.title(f\"% 'Unknown' by {group_col.title()}\")\n",
    "        fig.savefig(eda_dir / f\"02_unknown_heat_{tag}.png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    # Figure – Boxplots for campaign/previous/pdays_non999\n",
    "    for col, label in [('campaign','campaign'),('previous','previous'),('pdays_non999','pdays_non999')]:\n",
    "        if col in dfe.columns:\n",
    "            fig = plt.figure()\n",
    "            series = dfe[col].dropna()\n",
    "            plt.boxplot(series.values, vert=True, showfliers=True)\n",
    "            plt.title(f\"Boxplot: {label}\")\n",
    "            plt.ylabel(col)\n",
    "            fig.savefig(eda_dir / f\"03_boxplot_{label}.png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "    # Figure – Correlation matrix (numeric)\n",
    "    numeric_cols = dfe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    corr = dfe[numeric_cols].corr()\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(corr.values, aspect='auto')\n",
    "    plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(numeric_cols)), numeric_cols)\n",
    "    plt.colorbar(label=\"Correlation\")\n",
    "    plt.title(\"Correlation Matrix (Numeric Features)\")\n",
    "    fig.savefig(eda_dir / \"04_correlation_matrix.png\", dpi=150, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Scenario: Modern Macros\n",
    "# -----------------------------------------------------------------------------\n",
    "def apply_modern_macros(df: pd.DataFrame,\n",
    "                        euribor3m=None, cons_conf_idx=None,\n",
    "                        cons_price_idx=None, emp_var_rate=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply modern macroeconomic values (2023-2025) to dataset\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if euribor3m is not None and \"euribor3m\" in out.columns:\n",
    "        out[\"euribor3m\"] = euribor3m\n",
    "    if cons_conf_idx is not None and \"cons.conf.idx\" in out.columns:\n",
    "        out[\"cons.conf.idx\"] = cons_conf_idx\n",
    "    if cons_price_idx is not None and \"cons.price.idx\" in out.columns:\n",
    "        out[\"cons.price.idx\"] = cons_price_idx\n",
    "    if emp_var_rate is not None and \"emp.var.rate\" in out.columns:\n",
    "        out[\"emp.var.rate\"] = emp_var_rate\n",
    "    return out\n",
    "\n",
    "def score_scenarios(pipe: Pipeline, X_test: pd.DataFrame, y_test: np.ndarray,\n",
    "                    modern_df: pd.DataFrame, outdir: Path, tag: str):\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    # Historic\n",
    "    proba_hist = pipe.predict_proba(X_test)[:, 1]\n",
    "    m_hist = business_point_metrics(y_test, proba_hist, top_frac=TOP_FRACTION)\n",
    "    # Modern (align columns)\n",
    "    modern_X_test = modern_df.loc[X_test.index, X_test.columns]\n",
    "    proba_mod = pipe.predict_proba(modern_X_test)[:, 1]\n",
    "    m_mod = business_point_metrics(y_test, proba_mod, top_frac=TOP_FRACTION)\n",
    "\n",
    "    # Save comparison\n",
    "    comp = pd.DataFrame({\"y_true\": y_test, \"p_hist\": proba_hist, \"p_modern\": proba_mod}, index=X_test.index)\n",
    "    comp.to_csv(outdir / f\"proba_hist_vs_modern_{tag}.csv\", index=False)\n",
    "    pd.DataFrame([{\"scenario\": \"historic\", **m_hist},\n",
    "                  {\"scenario\": \"modern\", **m_mod}]).to_csv(outdir / f\"scenario_metrics_{tag}.csv\", index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Export Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def to_csv_xlsx(df: pd.DataFrame, base_path: Path):\n",
    "    base_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(base_path.with_suffix(\".csv\"), index=False)\n",
    "    try:\n",
    "        import openpyxl  # noqa: F401\n",
    "        df.to_excel(base_path.with_suffix(\".xlsx\"), index=False)\n",
    "    except Exception:\n",
    "        pass  # CSV is sufficient for marking\n",
    "\n",
    "def export_clean_and_splits(df: pd.DataFrame, X_train, X_test, data_dir: Path):\n",
    "    to_csv_xlsx(df, data_dir / \"cleaned_full\")\n",
    "    cleaned_train = df.loc[X_train.index].copy()\n",
    "    cleaned_test  = df.loc[X_test.index].copy()\n",
    "    to_csv_xlsx(cleaned_train, data_dir / \"cleaned_train\")\n",
    "    to_csv_xlsx(cleaned_test,  data_dir / \"cleaned_test\")\n",
    "    splits_idx = pd.concat([\n",
    "        pd.DataFrame({\"index\": X_train.index, \"split\": \"train\"}),\n",
    "        pd.DataFrame({\"index\": X_test.index,  \"split\": \"test\"})\n",
    "    ]).sort_values(\"index\").reset_index(drop=True)\n",
    "    splits_idx.to_csv(data_dir / \"splits_index.csv\", index=False)\n",
    "\n",
    "def export_ml_ready(pipe_lr: Pipeline, pipe_rf: Pipeline,\n",
    "                    X_train, X_test, y_train, y_test, ml_dir: Path):\n",
    "    ml_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def transform_and_save(pipe, X_tr, X_te, prefix: str):\n",
    "        pre = pipe.named_steps[\"pre\"]\n",
    "        Xtr_mat = pre.transform(X_tr)\n",
    "        Xte_mat = pre.transform(X_te)\n",
    "        try:\n",
    "            feat_names = pre.get_feature_names_out()\n",
    "        except Exception:\n",
    "            feat_names = [f\"f{i}\" for i in range(Xtr_mat.shape[1])]\n",
    "        Xtr_df = pd.DataFrame(Xtr_mat, index=X_tr.index, columns=feat_names)\n",
    "        Xte_df = pd.DataFrame(Xte_mat, index=X_te.index, columns=feat_names)\n",
    "        to_csv_xlsx(Xtr_df, ml_dir / f\"{prefix}_X_train\")\n",
    "        to_csv_xlsx(Xte_df, ml_dir / f\"{prefix}_X_test\")\n",
    "        with open(ml_dir / f\"feature_names_{prefix}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for n in feat_names:\n",
    "                f.write(str(n) + \"\\n\")\n",
    "\n",
    "    transform_and_save(pipe_rf, X_train, X_test, prefix=\"rf\")  # unscaled numerics\n",
    "    transform_and_save(pipe_lr, X_train, X_test, prefix=\"lr\")  # scaled numerics\n",
    "\n",
    "    # Targets\n",
    "    pd.DataFrame({\"y\": y_train}, index=X_train.index).to_csv(ml_dir / \"y_train.csv\", index=True)\n",
    "    pd.DataFrame({\"y\": y_test},  index=X_test.index).to_csv(ml_dir / \"y_test.csv\",  index=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            Train & Evaluate (one model)\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_and_evaluate_one(\n",
    "    algo: str, X_train, y_train, X_test, y_test, outdir: Path, \n",
    "    top_frac: float = TOP_FRACTION, use_resampling: str = None\n",
    ") -> Pipeline:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create appropriate pipeline\n",
    "    pipe = make_pipeline(pd.DataFrame(X_train, columns=X_train.columns), algo=algo, use_resampling=use_resampling)\n",
    "    \n",
    "    # If XGB, compute scale_pos_weight from training split and set it before fit\n",
    "    if algo == \"xgb\" and HAS_XGB:\n",
    "        pos = int(np.asarray(y_train).sum())\n",
    "        neg = len(y_train) - pos\n",
    "        spw = (neg / max(1, pos))\n",
    "        pipe.named_steps[\"clf\"].set_params(scale_pos_weight=spw)\n",
    "\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    proba = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "    # Core and business-point metrics\n",
    "    metrics_thresh05 = eval_classification(y_test, proba, threshold=0.5)\n",
    "    metrics_business = business_point_metrics(y_test, proba, top_frac=top_frac)\n",
    "\n",
    "    # Decile table\n",
    "    deciles = make_decile_table(y_test, proba, n_deciles=10)\n",
    "\n",
    "    # Model tag for naming\n",
    "    model_tag = algo\n",
    "    if use_resampling:\n",
    "        model_tag = f\"{algo}_{use_resampling}\"\n",
    "\n",
    "    # Save numeric outputs\n",
    "    pd.DataFrame([{\n",
    "        \"model\": model_tag,\n",
    "        **metrics_thresh05,\n",
    "        **{f\"business_{k}\": v for k, v in metrics_business.items()}\n",
    "    }]).to_csv(outdir / f\"metrics_{model_tag}.csv\", index=False)\n",
    "\n",
    "    pd.DataFrame({\"model\": model_tag, \"y_true\": y_test, \"y_proba\": proba}).to_csv(\n",
    "        outdir / f\"predictions_{model_tag}.csv\", index=False\n",
    "    )\n",
    "    deciles.to_csv(outdir / f\"deciles_{model_tag}.csv\", index=False)\n",
    "\n",
    "    # Plots\n",
    "    plot_roc(y_test, proba, f\"{model_tag.upper()} – ROC\", outdir / f\"{model_tag}_roc.png\")\n",
    "    plot_pr(y_test, proba, f\"{model_tag.upper()} – Precision-Recall\", outdir / f\"{model_tag}_pr.png\")\n",
    "    plot_lift(deciles, f\"{model_tag.upper()} – Lift by Decile\", outdir / f\"{model_tag}_lift.png\")\n",
    "    \n",
    "    # Cumulative capture plot\n",
    "    dec_copy = deciles.copy()\n",
    "    dec_copy[\"cum_capture_rate\"] = dec_copy[\"bin_capture_rate\"].cumsum() / dec_copy[\"bin_capture_rate\"].sum()\n",
    "    plt.figure()\n",
    "    plt.plot(dec_copy[\"decile\"], dec_copy[\"cum_capture_rate\"], marker=\"o\")\n",
    "    plt.xlabel(\"Decile (1 = highest score)\"); plt.ylabel(\"Cumulative Capture Rate\")\n",
    "    plt.title(f\"{model_tag.upper()} – Cumulative Capture\")\n",
    "    plt.tight_layout(); plt.savefig(outdir / f\"{model_tag}_cum_capture.png\", dpi=160); plt.close()\n",
    "    \n",
    "    plot_calibration(y_test, proba, f\"{model_tag.upper()} – Calibration\", outdir / f\"{model_tag}_calibration.png\")\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"\\n=== {model_tag.upper()} ===\")\n",
    "    print(\"AUC:\", metrics_thresh05[\"auc\"], \"PR-AUC:\", metrics_thresh05[\"pr_auc\"])\n",
    "    print(f\"At {int(top_frac*100)}% calling budget: \"\n",
    "          f\"capture={metrics_business['decile_capture_rate']:.3f}, \"\n",
    "          f\"lift={metrics_business['decile_lift']:.2f}, \"\n",
    "          f\"threshold={metrics_business['threshold_at_top_frac']:.4f}\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "#                            MAIN - COMPLETE IMPLEMENTATION\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    OUTDIR.mkdir(exist_ok=True, parents=True)\n",
    "    (OUTDIR / \"figures\").mkdir(exist_ok=True, parents=True)\n",
    "    (OUTDIR / \"models\").mkdir(exist_ok=True, parents=True)\n",
    "    (OUTDIR / \"eda\").mkdir(exist_ok=True, parents=True)\n",
    "    (OUTDIR / \"data\").mkdir(exist_ok=True, parents=True)\n",
    "    (OUTDIR / \"ml_ready\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Load raw and clean\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    raw = pd.read_csv(RAW_CSV_PATH, sep=\";\")\n",
    "    df = clean_dataset(raw)\n",
    "\n",
    "    # EDA (core figures only, as referenced in the report)\n",
    "    print(\"Running EDA...\")\n",
    "    run_eda_and_save(df, OUTDIR / \"eda\")\n",
    "\n",
    "    # Split\n",
    "    y = df[\"y\"].astype(int).values\n",
    "    X = df.drop(columns=[\"y\"])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, stratify=y, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Export cleaned + splits (for Appendix / reproducibility)\n",
    "    print(\"Exporting cleaned data and splits...\")\n",
    "    export_clean_and_splits(df, X_train, X_test, OUTDIR / \"data\")\n",
    "\n",
    "    # ---------------- Models ----------------\n",
    "    print(\"\\nTraining models...\")\n",
    "\n",
    "    # Logistic Regression (baseline)\n",
    "    print(\"\\n--- Logistic Regression Models ---\")\n",
    "    lr_dir = OUTDIR / \"models\" / \"lr\"\n",
    "    lr_pipe = train_and_evaluate_one(\"lr\", X_train, y_train, X_test, y_test, lr_dir)\n",
    "\n",
    "    # Logistic Regression (ROS)\n",
    "    if HAS_IMBLEARN:\n",
    "        lr_ros_dir = OUTDIR / \"models\" / \"lr_ros\"\n",
    "        lr_ros_pipe = train_and_evaluate_one(\"lr\", X_train, y_train, X_test, y_test, lr_ros_dir, use_resampling=\"ros\")\n",
    "    else:\n",
    "        lr_ros_pipe = None\n",
    "\n",
    "    # Random Forest (baseline)\n",
    "    print(\"\\n--- Random Forest Models ---\")\n",
    "    rf_dir = OUTDIR / \"models\" / \"rf\"\n",
    "    rf_pipe = train_and_evaluate_one(\"rf\", X_train, y_train, X_test, y_test, rf_dir)\n",
    "\n",
    "    # Random Forest (ROS)\n",
    "    if HAS_IMBLEARN:\n",
    "        rf_ros_dir = OUTDIR / \"models\" / \"rf_ros\"\n",
    "        rf_ros_pipe = train_and_evaluate_one(\"rf\", X_train, y_train, X_test, y_test, rf_ros_dir, use_resampling=\"ros\")\n",
    "    else:\n",
    "        rf_ros_pipe = None\n",
    "\n",
    "    # XGBoost\n",
    "    print(\"\\n--- XGBoost Models ---\")\n",
    "    if HAS_XGB:\n",
    "        xgb_dir = OUTDIR / \"models\" / \"xgb\"\n",
    "        xgb_pipe = train_and_evaluate_one(\"xgb\", X_train, y_train, X_test, y_test, xgb_dir)\n",
    "    else:\n",
    "        xgb_pipe = None\n",
    "        print(\"XGBoost not available - skipping\")\n",
    "\n",
    "    # Export ML-ready matrices (LR scaled, RF unscaled), plus targets\n",
    "    print(\"\\nExporting ML-ready datasets...\")\n",
    "    export_ml_ready(lr_pipe, rf_pipe, X_train, X_test, y_train, y_test, OUTDIR / \"ml_ready\")\n",
    "\n",
    "    # ---------------- Scenarios ----------------\n",
    "    print(\"\\nRunning scenario analysis with modern macroeconomic data...\")\n",
    "    modern_df = apply_modern_macros(\n",
    "        df,\n",
    "        euribor3m=MODERN_MACROS['euribor3m'],\n",
    "        cons_conf_idx=MODERN_MACROS['cons.conf.idx'],\n",
    "        cons_price_idx=MODERN_MACROS['cons.price.idx'],\n",
    "        emp_var_rate=MODERN_MACROS['emp.var.rate']\n",
    "    )\n",
    "\n",
    "    # Use all model variants for scenario check\n",
    "    score_scenarios(lr_pipe, X_test, y_test, modern_df, OUTDIR / \"scenario_lr\", tag=\"lr\")\n",
    "    score_scenarios(rf_pipe, X_test, y_test, modern_df, OUTDIR / \"scenario_rf\", tag=\"rf\")\n",
    "\n",
    "    if lr_ros_pipe is not None:\n",
    "        score_scenarios(lr_ros_pipe, X_test, y_test, modern_df, OUTDIR / \"scenario_lr_ros\", tag=\"lr_ros\")\n",
    "    if rf_ros_pipe is not None:\n",
    "        score_scenarios(rf_ros_pipe, X_test, y_test, modern_df, OUTDIR / \"scenario_rf_ros\", tag=\"rf_ros\")\n",
    "    if xgb_pipe is not None:\n",
    "        score_scenarios(xgb_pipe, X_test, y_test, modern_df, OUTDIR / \"scenario_xgb\", tag=\"xgb\")\n",
    "\n",
    "    # ---------------- Summary ----------------\n",
    "    print(\"\\nGenerating performance summary...\")\n",
    "    generate_performance_summary(OUTDIR)\n",
    "\n",
    "    print(f\"\\n✅ Complete! All artifacts written to: {OUTDIR.resolve()}\")\n",
    "    print(f\"Models trained: LR, LR+ROS, RF, RF+ROS, XGBoost\")\n",
    "    print(f\"Scenarios tested: Historic (2008-2013) vs Modern (2023-2025)\")\n",
    "\n",
    "\n",
    "def generate_performance_summary(outdir: Path):\n",
    "    \"\"\"Generate the comprehensive performance summary table from the report\"\"\"\n",
    "    summary_data = []\n",
    "\n",
    "    model_dirs = [\n",
    "        (\"Logistic Regression\", \"lr\", \"models/lr/metrics_lr.csv\"),\n",
    "        (\"Logistic Regression (ROS)\", \"lr_ros\", \"models/lr_ros/metrics_lr_ros.csv\"),\n",
    "        (\"Random Forest\", \"rf\", \"models/rf/metrics_rf.csv\"),\n",
    "        (\"Random Forest (ROS)\", \"rf_ros\", \"models/rf/metrics_rf_ros.csv\"),\n",
    "        (\"XGBoost\", \"xgb\", \"models/xgb/metrics_xgb.csv\")\n",
    "    ]\n",
    "\n",
    "    for model_name, model_code, metric_path in model_dirs:\n",
    "        full_path = outdir / metric_path\n",
    "        if full_path.exists():\n",
    "            try:\n",
    "                metrics = pd.read_csv(full_path).iloc[0].to_dict()\n",
    "                summary_data.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Dataset\": \"Historic\",\n",
    "                    \"ROC-AUC\": metrics.get(\"auc\", np.nan),\n",
    "                    \"PR-AUC\": metrics.get(\"pr_auc\", np.nan),\n",
    "                    \"Capture@10%\": metrics.get(\"business_decile_capture_rate\", np.nan),\n",
    "                    \"Top-decile lift\": metrics.get(\"business_decile_lift\", np.nan),\n",
    "                    \"Precision@0.5\": metrics.get(\"precision@0.5\", np.nan),\n",
    "                    \"Recall@0.5\": metrics.get(\"recall@0.5\", np.nan),\n",
    "                    \"Brier Score\": metrics.get(\"brier_score\", np.nan)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load metrics for {model_name}: {e}\")\n",
    "\n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(outdir / \"performance_summary.csv\", index=False)\n",
    "        print(\"Performance summary saved to: performance_summary.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
